# Техническая документация

**Проект:** AI Дебаты (LM Studio)

**Версия:** 1.0

**Дата:** 31 августа 2025

**Автор:** (проект пользователя)

---

# Описание проекта

Скрипт реализует локальную систему организации дебатов между двумя LLM (локальными инстансами LM Studio), запущенными на отдельных машинах/портах. Проект предоставляет два режима работы:

* **Бесконечные дебаты (infinite\_debate)** — модели по очереди формируют развёрнутые реплики на заданную тему до тех пор, пока не будет вызвана остановка или пока не накопится серия невалидных ответов.
* **Синхронный режим (synchronous\_mode)** — обе модели отвечают параллельно на один и тот же вопрос (опрос в один ход).

Проект включает механизмы валидации ответов, логирования, мониторинга состояния ботов и простую модерацию в ходе дебатов.

---

# Быстрый старт (установка и запуск)

1. **Требования**

   * Python 3.10+
   * Библиотеки (pip install):

     * requests
     * colorama
     * concurrent.futures (входит в stdlib)
     * logging (stdlib)

   Пример установки:

   ```bash
   pip install requests colorama
   ```

2. **Конфигурация**

   * В коде находится словарь `BOTS` с адресами локальных LM Studio API:

     ```python
     BOTS = {
        "Bot1": {"url": "http://192.168.8.87:12345/v1/chat/completions"},
        "Bot2": {"url": "http://192.168.8.89:1234/v1/chat/completions"}
     }
     ```
   * Убедитесь, что эти URL доступны с машины, где запускается скрипт.

3. **Запуск**

   ```bash
   python your_script.py
   ```

   * При запуске скрипт автоматически проверит доступность `BOTS` и отобразит меню.

---

# Архитектура и компоненты

## Основные файлы

* `main.py` — основной исполняемый файл (или имя, под которым вы сохранили скрипт), содержит весь функционал.
* `system.log` — файл системных логов (при нескольких запусках дописывается).
* `<mode>_dialog.log` — лог диалога конкретного режима (в режиме 'w', перезаписывается при старте).

## Компоненты (модули/функции)

* `check_bots_status()` — проверка доступности ботов и получение информации о моделях.
* `format_time(seconds)` — формирует читаемое представление времени ответа.
* `setup_logging(mode)` — настраивает логгирование для указанного режима.
* `spinner(message, stop_event)` — визуальный индикатор ожидания ответа бота (CLI).
* `ask_ai(bot_name, messages, temperature=0.7)` — отправляет POST-запрос к LM Studio, получает ответ и логирует метаданные.
* `is_valid_response(response, previous_responses)` — проверяет реплику на содержание, повторы, длину и целостность.
* `infinite_debate()` — цикл бесконечных дебатов с модерацией и проверкой стопа.
* `synchronous_mode()` — параллельные запросы ботам на один вопрос.
* `main()` — меню и точка входа.

---

# Подробное описание функционала (функции)

> Далее описания оформлены по шаблону: **Назначение** / **Входные параметры** / **Выход** / **Побочные эффекты / Замечания**.

## `format_time(seconds)`

* **Назначение:** Преобразует секунды в читаемый формат (секунды/минуты).
* **Вход:** `seconds` (float)
* **Выход:** строка вида `"12.34 сек"` или `"1 мин 12.34 сек"`.
* **Побочный эффект:** Нет.

## `setup_logging(mode)`

* **Назначение:** Конфигурирует логгер: `system.log` (append) и `<mode>_dialog.log` (перезапись).
* **Вход:** `mode` — строка, используется в имени файла диалога.
* **Выход:** Нет (хотя настраивает глобальный логгер).
* **Побочные эффекты:** Создаёт/перезаписывает `<mode>_dialog.log` и дописывает в `system.log`.
* **Замечания:** Текущая реализация очищает хендлеры: `logger.handlers.clear()`. При многократных вызовах логгера это безопасно, но возможна потеря хендлеров если другие части кода также настраивают лог.

## `is_valid_response(response, previous_responses)`

* **Назначение:** Проверяет, валидна ли реплика: не слишком короткая, не бессмысленная, не повторяет последние ответы.
* **Вход:**

  * `response` — строка ответа бота.
  * `previous_responses` — список ранее принятых ответов (используется последние 3 для сравнения).
* **Выход:** кортеж `(bool, reason)` — валидность, причина отклонения.
* **Побочные эффекты:** Нет.
* **Логика проверки:**

  * Длина ответа >= 20 символов
  * Сравнение схожести по множеству слов, запрещающее схожесть > 0.7 с последними тремя
  * Проверка по набору регексов для бессодержательных паттернов
  * Проверка на завершение знаками `. ! ?` если >5 слов
  * Минимум 5 слов
* **Замечания:** Подход со множеством слов может давать ложные срабатывания (рекомендуется использовать `difflib.SequenceMatcher` или n-граммы для более качественной оценки).

## `check_bots_status()`

* **Назначение:** Проверяет доступность каждого бота по его URL и пытается получить имя модели (`/models` endpoint).
* **Вход:** Использует глобальный `BOTS` словарь.
* **Выход:** Нет (модифицирует `BOTS` — добавляет `status`, `model`, `ip`).
* **Побочные эффекты:** Печатает состояние в консоль (с цветами).
* **Замечания:** Предполагается, что у LM Studio есть endpoint `/models` возвращающий JSON с полем `data`.

## `spinner(message, stop_event)`

* **Назначение:** Показывает анимацию ожидания в CLI.
* **Вход:** `message` (строка), `stop_event` (threading.Event)
* **Выход:** Нет.
* **Побочные эффекты:** Печатает в stdout; при параллельных потоках может конфликтовать с другими выводами.

## `ask_ai(bot_name, messages, temperature=0.7)`

* **Назначение:** Отправляет запрос к локальному LM Studio и получает ответ.
* **Вход:**

  * `bot_name` — ключ в `BOTS`.
  * `messages` — список сообщений в формате OpenAI-like (`role`, `content`).
  * `temperature` — параметр генерации.
* **Выход:** `(content, response_time, log_data)` или `(error_msg, 0, {})` в случае ошибки.
* **Побочные эффекты:** Запускает спиннер в отдельном потоке; логирует данные через `logging.info` строку `AI_RESPONSE_DATA: {...}`.
* **Формат отправляемого `payload`:**

  ```json
  {
    "model": "local-model",
    "messages": [...],
    "temperature": 0.7,
    "max_tokens": 300,
    "stop": ["\n\n", "###", "Пользователь:", "User:"]
  }
  ```
* **Замечания:**

  * В ответ предполагается структура OpenAI-совместимого формата: `choices[0].message.content` и `usage`.
  * Рекомендуется подставлять реальное имя модели в `model` (из `BOTS[bot_name]["model"]`).
  * Текущее ожидание запроса `timeout=600` — длинное; можно сделать конфигурируемым.

## `infinite_debate()`

* **Назначение:** Организует цикл дебатов между доступными ботами.
* **Вход:** Интерактивный — запрашивает тему у пользователя.
* **Выход:** Нет (печатает результат и логирует диалог).
* **Логика:**

  * Запускает `setup_logging('infinite_debate')`.
  * Получает список доступных ботов (`status == 'Online'`).
  * Формирует `messages` с `system` сообщением `DEBATE_PROMPT` (с подстановкой темы).
  * Цикл: по очереди добавляет `user`-тригер ("Теперь говорит {bot\_name}"), вызывает `ask_ai`, добавляет ответ в историю, валидирует через `is_valid_response`.
  * При 3 подряд невалидных ответах модератор добавляет сообщение с призывом вернуться к теме; при 5 невалидных в целом — завершается.
  * Остановка: отдельный демон-поток ждёт ввода `S` (ввод через `input()`), который ставит флаг `stop_requested`.
* **Замечания:**

  * Демон-поток, использующий `input()`, потенциально конфликтует с основным потоком ввода.
  * История `messages` растёт без ограничения — рекомендуется лимитировать число сообщений (например, 20 последних).

## `synchronous_mode()`

* **Назначение:** Задаёт вопрос — все доступные боты отвечают параллельно один раз.
* **Вход:** Ввод пользователя (вопрос) через `input()`.
* **Выход:** Печатает ответы и логирует.
* **Логика:**

  * Формирует `messages = [{"role": "user", "content": question}]`.
  * Через `ThreadPoolExecutor` отправляет `ask_ai` параллельно.
  * Полученные ответы выводит (цвет зависит от бота).
* **Замечания:**

  * Любые ошибки внутри `ask_ai` обрабатываются и печатаются.

## `main()`

* **Назначение:** Меню выбора режимов и запуск выбранного режима.
* **Логика:**

  * Вызывает `check_bots_status()` при старте.
  * Показывает меню (1 — бесконечные дебаты, 2 — синхронный режим, 3 — выход).
  * При выходе печатает прощание.

---

# Формат сообщений и взаимодействие с LM Studio

* Используется `messages` в стиле OpenAI Chat Completions API: массив объектов с `role` (`system`, `user`, `assistant`) и `content`.
* Отправляемый `payload` содержит `model`, `messages`, `temperature`, `max_tokens` и `stop`-токены.
* Ожидаемая структура ответа:

  ```json
  {
    "choices": [
      {"message": {"content": "..."}, "finish_reason": "..."}
    ],
    "usage": {"prompt_tokens": X, "completion_tokens": Y, "total_tokens": Z},
    "model": "..."
  }
  ```

---

# Механизмы модерации и валидации

1. **DEBATE\_PROMPT** — системное сообщение, задающее правила для участников дебатов (строгая логика, избегать повторов, развёрнутые ответы).
2. **Автоматическая проверка ответов (`is_valid_response`)** — отбрасывает короткие, бессодержательные или повторяющиеся ответы и те, что обрываются.
3. **Модератор** — встроенное поведение: при серии невалидных ответов в `infinite_debate` вставляется модераторское сообщение с требованием вернуться к теме.

---

# Логирование и аналитика

* `setup_logging(mode)` создаёт:

  * `system.log` — общий лог событий (append)
  * `<mode>_dialog.log` — лог диалога для текущего режима (перезаписывемый)
* В `ask_ai` формируется JSON-объект `AI_RESPONSE_DATA` и логируется через `logging.info`:

  * `bot`, `response_time`, `tokens_used`, `completion_tokens`, `prompt_tokens`, `timestamp`, `model`, `finish_reason`.
* В `infinite_debate` и `synchronous_mode` логируются принятые ответы через `logging.info`.

Рекомендации:

* Добавить ротацию логов (RotatingFileHandler) для `system.log`.
* Разнести логи: diagnostics / audit / dialogue для удобства.

---

# Параметры конфигурации и рекомендации

* **BOTS** — вынести в конфиг (`config.yml` или `config.json`) вместо хардкода.
* **TIMEOUT** — вынести параметр `requests` `timeout` в конфигурацию.
* **MAX\_HISTORY** — ограничить длину `messages` в `infinite_debate`, например `MAX_HISTORY = 20`.
* **RETRY\_POLICY** — добавить механизм повторов для сетевых ошибок (exponential backoff).
* **MODEL\_OVERRIDE** — подставлять реальное имя модели в payload: `model = BOTS[bot_name].get("model")`.
* **LOG\_LEVEL** — сделать конфигурируемым.

---

# Параллелизм и потоковая безопасность

* **Spinner** — запускается в отдельном потоке для каждого `ask_ai`. При множественных параллельных запросах вывод может смешиваться.
* **Stop thread** — в `infinite_debate` использован демон-поток с `input()`. Потенциальные гонки: `input()` в нескольких потоках вызвать нельзя — Возможен конфликт с основным потоком, если одновременно вызываются другие `input()`.
* **ThreadPoolExecutor** — используется корректно для параллельных запросов в `synchronous_mode`.

Рекомендации:

* Убрать `input()` в фоновых потоках; вместо этого ловить `KeyboardInterrupt` (Ctrl+C) или использовать неблокирующее чтение stdin.
* Держать общий `print` через отдельную очередь (`queue.Queue`) и один поток-вывода, чтобы избежать смешивания текста.

---

# Ошибки и обработка исключений

**Текущая реализация:**

* В `ask_ai` при ошибках возвращается сообщение вида `"BotName Connection Error: ..."`, при этом таймауты/код ошибки логируются.
* В `synchronous_mode` исключения при `future.result()` ловятся и печатаются.

**Рекомендации:**

* Добавить более явную стратегию retry (с лимитом). При сетевых проблемах — пометить бота как `Offline` и перестать опрашивать на N минут (circuit breaker).
* Валидировать JSON-ответы и обрабатывать `KeyError`/`IndexError`.

---

# Безопасность и приватность

* Скрипт общается по HTTP с локальными хостами (LAN). Убедитесь, что ваши LM Studio инстансы находятся в защищённой сети.
* Не включайте секреты/ключи в код. Если LM Studio требует аутентификацию — храните токены в переменных окружения или в защищённом конфиге.
* Санитизируйте пользовательский ввод, если будете расширять проект до сетевого сервиса.

---

# Тестирование

## Ручное

1. Запустите LM Studio или имитируйте endpoint с тестовым ответом (напр. `http.server` + простой обработчик returning static JSON).
2. Выполните `python main.py` и проверьте меню.
3. Проверьте `synchronous_mode`: задайте вопрос и проверьте одновременные ответы.
4. Проверьте `infinite_debate`: задайте тему и дайте ботам «поработать» несколько циклов.

## Юнит-тесты (рекомендации)

* Тесты для `is_valid_response` с набором позитивных/негативных примеров.
* Мокирование `requests.post` и тестирование `ask_ai` с корректными и ошибочными ответами.
* Тесты для `check_bots_status` с моками `/models`.

---

# Траблшутинг (частые проблемы и решения)

* **Ошибка подключения (ConnectionError / Timeout):** Проверьте доступность IP/порта, firewall, правильность URL (`/v1/chat/completions`).
* **Непредвиденная структура ответа (KeyError):** Логируйте `response.text` и убедитесь, что LM Studio возвращает OpenAI-like формат.
* **Спиннер мешает выводу:** Запускайте скрипт в `infinite_debate` без спиннера или отключайте его в `synchronous_mode`.
* **Дублирование логов/хендлеров:** Убедитесь, что `setup_logging` корректно переиспользует или очищает хендлеры.

---

# Расширения и возможные улучшения

1. Вынести конфигурацию в `config.yml`.
2. Добавить веб-интерфейс (Flask/FastAPI + WebSocket) для визуализации дебатов и управления.
3. Добавить поддержку более чем 2 ботов и динамическое изменение набора участников.
4. Повысить качество валидации (n-грам, sequence matcher, векторные эмбеддинги для семантического сравнения).
5. Собрать метрики (Prometheus) и дашборд (Grafana) для мониторинга задержек и качества ответов.
6. Интеграция с системой аутентификации LM Studio (если требуется токен).
7. Возможность сохранять сессии в базе данных (SQLite/Postgres) и экспорт в JSON/CSV.

---

# Примеры

## Пример `BOTS` конфигурации

```python
BOTS = {
    "Bot1": {"url": "http://192.168.8.87:12345/v1/chat/completions"},
    "Bot2": {"url": "http://192.168.8.89:1234/v1/chat/completions"}
}
```

## Пример payload к LM Studio

```json
{
  "model": "gpt-local-1",
  "messages": [
    {"role": "system", "content": "Вы участник дебатов..."},
    {"role": "user", "content": "Теперь говорит Bot1"}
  ],
  "temperature": 0.7,
  "max_tokens": 300,
  "stop": ["\n\n", "###", "Пользователь:", "User:"]
}
```

## Пример лог-строки (AI\_RESPONSE\_DATA)

```
AI_RESPONSE_DATA: {"bot": "Bot1", "response_time": 1.234, "tokens_used": 123, "completion_tokens": 100, "prompt_tokens": 23, "timestamp": "2025-08-31T10:00:00", "model": "gpt-local-1", "finish_reason": "stop"}
```

---

# Чек-лист для развёртывания

* [ ] LM Studio запущен и доступен на указанных адресах
* [ ] Установлены зависимости Python
* [ ] Настроен конфиг `BOTS` или `config.yml`
* [ ] Протестирован endpoint `/models` (возвращает `data[0].id`)
* [ ] Запуск `python main.py` проходит без ошибок

---

